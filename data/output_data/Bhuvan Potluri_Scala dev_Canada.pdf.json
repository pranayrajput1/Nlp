{
    "name": "Bhuvan Potluri",
    "email": "bhuvanpotluride2_s6r@indeedemail.com",
    "mobile_number": "438-725-6769",
    "skills": [
        "Nosql",
        "Zookeeper",
        "Scheduling",
        "Troubleshooting",
        "Technical",
        "Postgresql",
        "Test cases",
        "Documentation",
        "Pycharm",
        "Pl/sql",
        "Db2",
        "Windows",
        "Etl",
        "Workflows",
        "Database",
        "Health",
        "Queries",
        "Sql",
        "Agile",
        "Pdf",
        "Logistics",
        "Sdlc",
        "Coding",
        "Servers",
        "Operating systems",
        "Test plans",
        "Unix",
        "Sales",
        "Ubuntu",
        "Docker",
        "Automation",
        "Scripting",
        "Administration",
        "Writing",
        "Adobe",
        "Cloud",
        "Rest",
        "Pyspark",
        "Shell",
        "Hive",
        "Excel",
        "Scala",
        "Drafting",
        "Datasets",
        "Mysql",
        "Python",
        "Profiling",
        "Conversion",
        "Acquisition",
        "Analytics",
        "Oracle",
        "Interactive",
        "Modeling",
        "Hbase",
        "Spark",
        "Programming",
        "Anaconda",
        "Testing",
        "Analyze",
        "System",
        "Audit",
        "Visual",
        "Architecture",
        "Tableau",
        "Apis",
        "Vendors",
        "Scrum",
        "Reporting",
        "Sql server",
        "Process",
        "Json",
        "Algorithms",
        "Jupyter",
        "Transport",
        "Analysis",
        "Design",
        "Strategy",
        "Specifications",
        "Hadoop",
        "Sap",
        "Debugging",
        "Warehouse",
        "Xml",
        "Linux",
        "Reports",
        "Aws",
        "Salesforce"
    ],
    "college_name": null,
    "degree": null,
    "designation": [
        "Big Data Engineer / ETL Developer",
        "Data Engineer",
        "SQL Data Analyst",
        "Data Engineer/AWS Engineer"
    ],
    "experience": [
        "Big Data Engineer / ETL Developer",
        "ADP - Toronto, ON",
        "April 2020 to Present",
        "Canada",
        "Roles &Responsibilities",
        "\u2022  Involved  in  requirements  gathering,  analysis,  design,  development,  change  management  and",
        "deployment.",
        "\u2022 Assisted Business Analyst with drafting the requirements, implementing design and development of",
        "various components of ETL for various applications.",
        "\u2022  Monitor  and  modify  daily  &  on  demand  batch  jobs  that  load  warehouse  tables  from  various  source",
        "systems like Oracle, Sql server, Salesforce.",
        "\u2022 Fix sessions that result in undesired load data in lower & higher environments for both Data warehouse",
        "and DataMart's.",
        "\u2022 Define match rules, columns for match and merge process.",
        "\u2022 Performed Data Cleansing, Address correction using IDQ.",
        "\u2022 Built profiling, cleansing and validation plans using IDQ.",
        "\u2022 Development on existing mappings that acquired extended scope to accommodate new logistics and",
        "perform regressive testing before migrating it to higher environments.",
        "\u2022 Extracted data from heterogeneous sources and performed complex business logic on network data to",
        "normalize raw data which can be utilized by BI teams to detect anomalies.",
        "\u2022 Worked with Spark Ecosystem using Scala and Hive Queries on different data formats like Text file.",
        "\u2022 Developed Spark code using Scala and Spark-SQL for faster processing and testing.",
        "\u2022 Using Spark-Streaming APIs to perform transformations and actions on the fly for building the common",
        "learner data model which gets the data from Kafka in near real time",
        "\u2022 Involved in the development of real time streaming applications using Kafka, Hive,PySpark, Apache",
        "Flink on distributed Hadoop Cluster.",
        "\u2022 Performs debugging, troubleshooting, modifications and unit testing of integration solutions",
        "\u2022 Perform unit and integration testing and document test strategy and results",
        "\u2022 Work closely with teams from other enterprise software vendors that are being integrated",
        "\u2022 Use innovation to improve operational processes and performance, making sure data is of the highest",
        "quality, build and unit test integration components",
        "\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics.",
        "\u2022 Developed common Flink module for serializing and deserializing AVRO data by applying schema.",
        "\u2022 Implemented layered architecture for Hadoop to modularize design.",
        "Developed framework scripts to enable quick development.",
        "\u2022 Used Microservice architecture with Spring Boot based services interacting through a combination of",
        "REST to build, test and deploy identity Microservices",
        "\u2022  Developed  Spark  scripts  using  Python  on  AWS  EMR  for  Data  Aggregation,  Validation  and  Adhoc",
        "querying.",
        "\u2022 Data Extraction, aggregations and consolidation of Adobe data within AWS Glue using PySpark",
        "\u2022 Developed CI-CD pipeline to automate build and deploy to Dev, QA, and production environments.",
        "\u2022  Supported  production  jobs  and  developed  several  automated  processes  to  handle  errors  and",
        "notifications. Also, tuned performance of slow jobs by improving design",
        "and configuration changes of PySpark jobs.",
        "\u2022 Created standard report Subscriptions and Data Driven Report Subscriptions.",
        "Environment:  Hadoop,  Informatica  9.8.1/10.2,  IDQ,  Salesforce,  Embarcadero  ER  Studio  Map  Reduce,",
        "Spark, Spark MLLib,Kafka,Flink,PIG, Hive, AWS,Tableau, SQL, PostgresSQL, Python, PySpark, SQL Server",
        "2012, T-SQL, CI-CD, Git, XML, Splunk.",
        "Data Engineer",
        "MORGAN STANLEY - Quebec City, QC",
        "February 2019 to March 2020",
        "Canada",
        "Roles & Responsibilities",
        "\u2022 Involved in implementation of new statistical algorithms and operators on Hadoop and SQL platforms",
        "and  utilized  optimizations  techniques,  linear  regressions,  K-means  clustering,  Native  Bayes  and  other",
        "approaches.",
        "\u2022 Developed Spark batch job to automate creation/metadata update of external Hive table created on",
        "top of datasets residing in HDFS.",
        "\u2022 Developed Data Serialization spark common module for converting Complex objects into sequence bits",
        "by using AVRO, PARQUET, JSON, CSV formats.",
        "\u2022 Worked on ERModeling, Dimensional Modeling",
        "(StarSchema, snowflakeSchema), Data warehousing and OLAP tools.",
        "\u2022 Populated HDFS and PostgreSQL with huge amounts of data using Apache Kafka.",
        "\u2022 Develop disaster recovery and failover strategies for the data integration environments",
        "\u2022 Design, implement and perform administration functions for Enterprise data integration environments",
        "\u2022 Involved in migrating RESTful services to Microservices.",
        "\u2022  Developed  Microservices  to  handle  specific  business  logic  to  have  easy  maintainability  and  ease  of",
        "testing",
        "\u2022 Designed Batch Audit Process in batch\\shell script to monitor each ETL job along with reporting status",
        "which includes table name, start and finish time, number of rows loaded, status, etc.",
        "\u2022 Developed Spark jobs in PySpark to perform ETL from SQL Server to Hadoop.",
        "\u2022 Responsible for continuous monitoring and managing Elastic MapReduce (EMR) cluster through AWS",
        "console.",
        "\u2022 Design and Develop ETL Processes in AWS Glue to migrate Campaign data from external sources like",
        "S3, ORC/Parquet/Text Files into DataBase.",
        "\u2022 Worked with EMR, S3 and EC2 services in AWS cloud.",
        "\u2022 Migrating servers, databases, and applications from on premise to AWS.",
        "\u2022  Designed  and  implemented  data  acquisition,  ingestion,  Management  of  Hadoop  infrastructure  and",
        "other Analytics tools (Splunk, Tableau).",
        "\u2022 Working knowledge of build automation and CI/CD pipelines.",
        "\u2022 Developed python scripts to automate data ingestion pipeline for multiple data sources and deployed",
        "Apache NIFI in AWS.",
        "\u2022  Design  and  develop  Tableau  visualizations  which  include  preparing  Dashboards  using  calculations,",
        "parameters, calculated fields, groups, sets and hierarchies.",
        "Environment: Hadoop, Map Reduce, Spark, Spark MLLib, Kafka,NiFi, SQL, PIG, Hive, AWS, PostgresSQL,",
        "Python, PySpark, Microservices SQL Server 2012, T-SQL, CI-CD, Git, XML, Tableau.",
        "Data Engineer/AWS Engineer",
        "Hexaware - Chennai, Tamil Nadu",
        "August 2017 to December 2018",
        "India",
        "Roles &Responsibilities",
        "\u2022 Analyzing Functional Specifications Based on Project Requirement.",
        "\u2022 Ingested data from various data sources into Hadoop HDFS/Hive Tables using SQOOP, Flume, Kafka.",
        "\u2022 Developing Hive Queries for the user requirement.",
        "\u2022  Worked  on  multiple  POCs  in  Implementing  Data  Lake  for  Multiple  Data  Sources  ranging  from",
        "TeamCenter, SAP, Workday, Machine logs.",
        "\u2022 Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data.",
        "\u2022 Creates and owns the strategic roadmap for data integration across the enterprise",
        "\u2022 Strong understanding of Enterprise data integration patterns and techniques",
        "\u2022 Ability to validate data integration by developing and executing test plans",
        "\u2022 Planning, scheduling and implementing Oracle to MS SQL server migrations for in house applications",
        "and tools.",
        "\u2022 Designed and developed Micro Services business components using Spring",
        "\u2022 Worked on Solr Search Engine to index incident reports data and developed dash boards in Banana",
        "Reporting tool.",
        "\u2022 Integrated Tableau with Hadoop data source for building dashboard to provide various insights on sales",
        "of the organization.",
        "\u2022 Worked on Spark in building BI reports using Tableau. Tableau was integrated with Spark using Spark-",
        "SQL.",
        "\u2022 Developed Spark jobs using Scala and Python on top of Yarn/MRv2 for interactive and Batch Analysis.",
        "\u2022 Created multi-node Hadoop and Spark clusters in AWS instances to generate terabytes of data and",
        "stored it in AWS HDFS.",
        "\u2022 Developed work flows in Live Compare to Analyze SAP Data and Reporting.",
        "\u2022 Participated in daily scrum meetings and iterative development.",
        "\u2022 Search functionality for searching through millions of files of logistics groups.",
        "Environment:Hadoop, Hive, Sqoop, Spark, Kafka, Scala, MS SQL Server PDW, AWS.",
        "SQL Data Analyst",
        "Bluepal Solutions PVT LTD - Hyderabad, Telangana",
        "November 2016 to July 2017",
        "India",
        "Roles &Responsibilities",
        "\u2022 Designed and created Data Marts in data warehouse database",
        "\u2022 Implementations of MS SQL Server Management studio 2008 to create Complex Stored Procedures and",
        "Views using T-SQL.",
        "\u2022  Collecting  the  data  from  many  resources  and  converting  into  flat  text  files  with  comma  delimiter",
        "separator and importing the data to the SQL server for data manipulations.",
        "\u2022 Responsible for deploying reports to Report Manager and Troubleshooting for any errors during the",
        "execution.",
        "\u2022 Scheduled the reports to run on daily and weekly basis in Report Manager and emailing them to director",
        "and analyst to be reviewed in Excel Sheets.",
        "\u2022 Created several reports for claims handling which had to be exported out to PDF formats.",
        "\u2022 Analyzed business requirements and provided excellent and efficient solutions",
        "Environment: SQL Server 2008, Microsoft Visual Studio 2008, MS Office, SSRS"
    ],
    "company_names": [
        "Bluepal Solutions PVT LTD"
    ],
    "no_of_pages": 5,
    "total_experience": 5.25
}