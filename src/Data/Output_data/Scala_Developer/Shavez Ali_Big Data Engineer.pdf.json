{
    "name": "SHAVEZ ALI",
    "email": "shavej08ali@gmail.com",
    "mobile_number": "7503631114",
    "skills": [
        "Scala",
        "Workflow",
        "Ibm",
        "Hbase",
        "C++",
        "Nosql",
        "Json",
        "Sql server",
        "Python",
        "Writing",
        "Unix",
        "Pycharm",
        "Db2",
        "Sdlc",
        "Operations",
        "Scripting",
        "Agile",
        "Hive",
        "Cloud",
        "Technical skills",
        "Architecture",
        "Try",
        "Etl",
        "Design",
        "Shell",
        "Hadoop",
        "Pyspark",
        "Ui",
        "Database",
        "System",
        "Tfs",
        "Java",
        "Aws",
        "Sql",
        "C",
        "Hotel",
        "Analytical",
        "Communication",
        "Email",
        "Process",
        "Electronics",
        "Technical",
        "Travel",
        "Spark",
        "Analysis",
        "Conversion"
    ],
    "college_name": null,
    "degree": null,
    "designation": [
        "Software Engineer"
    ],
    "experience": [
        "Project Name:              GLLD(Global Large Loss Data System)",
        "Client:                           Crawford USA",
        "Environment:                AWS Glue, S3, Athena, SNS, Redshift and Pyspark Spark",
        "Role:                             Software Engineer",
        "Duration:                      Nov \u2013 2020 to present",
        "Project Description: Glld system report all the large loss Claim for all listed countries. Here we are created",
        "one pipeline to ingested data from the RDMS into S3 and top of this data created Athena tables with the help of",
        "crawler.  After that we apply all business transformation and make this into one standard form. In last step push all",
        "data into redshift.",
        "Project Name               : PostDQS (Post Data Quality Source)",
        "Client                            : BCD Travel USA",
        "Enviroment                   : Spark , Hbase, Scala, Cassandra, Hive, Sql Server",
        "Role                               : Software Engineer",
        "Duration                        : May \u2013 2019 to OCT- 2020",
        "Project Description: It a Post Trip ETL Project . We get all transaction data from Cassandra based on Dela",
        "and convert all pickup into canonical form then first apply HMF, BRMS and ETL mapping  rule in a same sequence",
        "form post this export same data into Cassandra , Hive and SQl server.",
        "Project Name               : DQT (Data Quality Tool)",
        "Client                            : BCD Travel USA",
        "Enviroment                   : Spark , Hbase, Scala, Cassandra, Hive,Python",
        "Role                               : Software Engineer",
        "Duration                        : Jun \u2013 2018 to May \u2013 2019",
        "Project Description: This is a Post Trip Project  and clean data for analysis pirpose. We are get data from",
        "two different resources like Zip file  and IBM DB2 then insert all data in hive table after that  convert  all data into",
        "Canonical model(In Object Form) and apply some rules(validation and Kie)  then store all data in Cassandra.",
        "Project  Name            : Import_Export(Data ingestion)",
        "Client                         : BCD Travel USA",
        "Enviroment                   :  Sqoop,shell script, Hive,Hbase, jenking.",
        "Role                               : Software Engineer.",
        "Duration                            : Jan \u2013 2018 to May \u2013 2018",
        "Project Description:",
        "This project import all the master data from Sql server and Denodo  into hive",
        "using Apache sqoop. Few tables have a direct load otherwise most table have incremental load based on",
        "Dela. After lookup with transaction data then export same data into sql servr. jenking  use to trigger this job.",
        "Project Name                     : TSH (Trip Source Hotel)",
        "Client                                 : BCD Travel USA",
        "Environment",
        "Role                            : Software Engineer",
        "Duration                      : Jan-2018 TO Aug 2018",
        ": HDFS, Hive, Spark sql, Hbase, JSON, Python, Scala",
        "Project Description :  A rich architecture of hive master-staging hbase integrated tables are used in",
        "this project. Process raw JSON in spark with currency conversion. VDB used for data process on UI.",
        "Extra-curricular Tasks",
        "\u2756  Participate in  Smart India HACKATHAN\u201917.",
        "\u2756  1ST Rank in HackerRank (sql server).",
        "\u2756  Active participate in HackerEarth.",
        "DECLARATION",
        "I here by declare that the above furnished information is true to the best of my knowledge and I will try my level",
        "best to rise your expectations if I am appointed.",
        "PLACE : \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026",
        "DATE :............................                                                                              (Shavez Ali)"
    ],
    "company_names": [
        "Shree Infosoft Pvt Ltd Gurugram"
    ],
    "no_of_pages": 4,
    "total_experience": 0.0
}