{
    "name": "AJAY RAUT",
    "email": "rautajay0701@gmail.com",
    "mobile_number": "9762550540",
    "skills": [
        "Computer science",
        "Testing",
        "Metrics",
        "English",
        "Reporting",
        "Python",
        "Writing",
        "Automation",
        "Data analysis",
        "Consulting",
        "Oracle",
        "Chemicals",
        "Mysql",
        "Profiling",
        "Operations",
        "Information management",
        "Hive",
        "Datasets",
        "Cloud",
        "Architecture",
        "Programming",
        "Etl",
        "Design",
        "Hadoop",
        "Pyspark",
        "Windows",
        "Database",
        "Sales",
        "System",
        "Java",
        "Aws",
        "Sql",
        "Mobile",
        "Servers",
        "Administration",
        "Logging",
        "Consulting services",
        "Queries",
        "Spark",
        "Analysis",
        "Distribution"
    ],
    "college_name": null,
    "degree": [
        "Diploma in Computer",
        "BE in Computer Science and Engineering(64.88%) from MSS\u2019s CET Jalna"
    ],
    "designation": null,
    "experience": [
        "Organization: Ellicium Solutions Pvt",
        "Ltd",
        "Client: US Cellular",
        "Duration: Oct 2019 \u2013 May 2020",
        "Role: Big Data and Spark Developer",
        "Project : USCC Datahub",
        "Technology Stack: Spark, AWS, Windows, Talend 7.0, SQL Developer, Hive,",
        "Oracle",
        "U.S. Cellular, is a mobile network operator which owns and operates the fifth-largest wireless",
        "telecommunications network in the United States, serving 5 million customers in 426 markets",
        "in 23 U.S. states. Ellicium was responsible for creating a Data Lake on AWS and migrating",
        "USCC's data from Oracle to Hive and validating the data in target system. My responsibilities",
        "in   this   project   include   developing   Talend   jobs   for   migrating   data   from   Oracle   to   Hive",
        "environment based on client\u2019s requirement.",
        "Roles and Responsibilities:",
        "\u25cf Data Profiling of the source data systems",
        "\u25cf Understand the requirements from the client through BRD.",
        "\u25cf Creating Spark applications to get data from source systems, apply business rules and",
        "transform them.",
        "\u25cf Loading large datasets into Hive tables using Spark Application.",
        "\u25cf Writing spark applications to ingest and perform complex operations on data. Logging",
        "errors encountered in Spark, analysing errors and fixing them",
        "\u25cf Also worked on Talend including Talend Administration Centre, Talend Studio,",
        "Standard jobs and Big Data jobs",
        "\u25cf Using Talend, create joblets used to pick up dynamic parameters from configuration",
        "tables. These included all the custom values that would have to be changed between",
        "environments.",
        "\u25cf Writing complex SQL Queries S2T mappings for Each entity. Writing DDL to create hive",
        "table for optimized performance Performing Unit testing and integration testing.",
        "\u25cf Data Validation after Data Ingestion.",
        "\u25cf Creating High Level Design Document for each entity.",
        "\u25cf Query optimization and ETL fine-tuning.",
        "Organization: Ellicium Solutions Pvt",
        "Ltd",
        "Client: KBR",
        "Duration: Aug 2019 \u2013 Oct 2019",
        "Role: Big Data and Spark Developer",
        "Project : Impact of NOTAM on flight delay",
        "Technology Stack: Spark, Java, Eclipse, Hive,Hadoop",
        "KBR partners with government and industry clients to and comprehensive solutions for a",
        "wide range of industries and companies all over the world. With a full portfolio of services,",
        "proprietary technologies and expertise ranging from, to mission critical services for",
        "aerospace and defense, to large scale energy and chemicals sites and to meeting the",
        "physical and digital infrastructure needs of governments and organizations across the",
        "globe.",
        "Roles and Responsibilities:",
        "\u25cf Data Profiling and requirement analysis.",
        "\u25cf Preparing data from large datasets to predict flight delays due to NOTAM.",
        "\u25cf Loading large datasets into hive tables using Spark Application.",
        "\u25cf Writing Spark applications using Java to ingest and perform complex operations on",
        "data.",
        "\u25cf Predicting impact of NOTAM on flight delay.",
        "Organization: EC Mobility Pvt. Ltd.",
        "Client: CMORE",
        "Duration: Sept 2017 \u2013 Aug 2019",
        "Role: Data Engineer",
        "Project : CMORE\u2019s Data Ingestion",
        "Technology Stack: Spark, Hadoop, Sqoop, Oracle, Hive, SQL",
        "CMORE has Lines of Businesses like Sales, Risks, Payment, Revenues etc. across the world",
        "divided country wise over multiple source servers. It has a large amount of data in various",
        "data sources like Relational Databases, Flat Files. In our project, we are ingesting and",
        "processing data and providing information to them for analytic purposes.",
        "Roles and Responsibilities:",
        "\u25cf Imported required tables from RDBMS (Oracle, SQLServer) to HDFS",
        "using Sqoop. Writing DDL to create hive table for optimized",
        "performance",
        "\u25cf Gathering required information for Data Ingestion.",
        "\u25cf Written Hive Queries for Data Analysis to meet the business",
        "requirements Data Validation after Data Ingestion.",
        "\u25cf Involved in creating Hive tables, loading with data, and writing hive",
        "queries.",
        "\u25cf Used Hive to analyse the partitioned and bucketed data and compute",
        "various metrics for reporting.",
        "\u25cf Used Hive optimization techniques during joins and best practices in",
        "writing hive scripts using HiveQL.",
        "\u25cf Importing data into HDFS and Hive using Sqoop.",
        "\u25cf Designed both Managed and External Hive Tables and Defined static",
        "and dynamic partitions as per requirement for optimized performance",
        "on production data sets."
    ],
    "company_names": null,
    "no_of_pages": 3,
    "total_experience": 2.67
}