{
    "name": "Kainth Big",
    "email": "aashishkainth96@gmail.com|",
    "mobile_number": "7009346720",
    "skills": [
        "Hive",
        "Big data",
        "Scala",
        "Communication",
        "C",
        "Engineering",
        "Hadoop",
        "Etl",
        "Oracle",
        "Process",
        "Falcon",
        "Shell",
        "Cloud",
        "Analysis",
        "Technical",
        "Electronics",
        "Db2",
        "Json",
        "Unix",
        "Scripting",
        "Nosql",
        "Spark",
        "Programming",
        "Tableau",
        "Training",
        "Agile",
        "Github",
        "Writing",
        "Certification",
        "System",
        "Scheduling",
        "Analytical",
        "English"
    ],
    "college_name": null,
    "degree": [
        "Bachelor of technology (Electronics and communication)"
    ],
    "designation": [
        "Hadoop Developer"
    ],
    "experience": [
        "March 2018 \u2013 Present                  System Engineer",
        "Tata Consultancy Services",
        "Technical Skill",
        "Big Data Technologies: HDFS, Spark, Hive, Sqoop",
        "Programming Languages: Scala, C",
        "Scripting Languages: Unix Shell scripting",
        "Other tools: Putty, Eclipse, GitHub, ETL, BitBucket",
        "Methodologies: Agile and Waterfall",
        "Project:  Bank of America",
        "Project: Falcon",
        "Role: Hadoop Developer",
        "Project Description:",
        "This project is regarding processing the raw data file by using spark application and then ingested the processed",
        "file from HDFS to Hive table and later files are generated as extracts using HIVE QL and sent to downstream. The",
        "Spark application code is developed using Scala in a Generic Extraction Framework (GEF)model, which processes",
        "data by referring the parameters in property file.",
        "Responsibilities:",
        "\u2022  Writing spark code to processed the raw data file and load the processed data file into HDFS path",
        "\u2022  Data Loading from HDFS path to hive external table.",
        "\u2022  Develop Hive Query Language (HQL) for the extracts generation for downstream",
        "\u2022  Autosysscheduling to run the framework with required frequency(daily/weekly/monthly/quarterly)  to",
        "handle various data ingestion..",
        "\u2022",
        "Involved in complete end to end deployment process in production.",
        "Project: Erica",
        "Role: Hadoop Developer",
        "Project Description:",
        "This project is regarding to ingest data from multiple sources DB2, Oracle, SQLServer, Teradata, Flat Files,",
        "Cassandra. Initially it was all written as single application. Due to increase in sources for analytical discovery, we",
        "developed a Common Ingestion Framework which connects the data sources and ingest the data into Hadoop",
        "after validation for analytical purpose. Framework is developed in such a way that it handles sources like, JSON,",
        "RDBMS, Flat file and NoSQL like Cassandra.",
        "After data ingested into Hadoop, it is visualized by Tableau team.",
        "Responsibilities:",
        "\u2022  Develop Hive Query Language (HQL) for the extracts generation for downstream.",
        "\u2022  Create new DDLs and make modifications for the new installs according to the requirement in the",
        "existing framework.",
        "\u2022  Autosys scheduling to run the framework with required frequency(daily/weekly/monthly/quarterly)  to",
        "handle various data ingestion.",
        "Involved in complete end to end deployment process in production.",
        "\u2022",
        "POC: Converting Multidimensional JSON file to flat-file using Spark and load into the hive table",
        "Role: Hadoop Developer",
        "Project Description:"
    ],
    "company_names": null,
    "no_of_pages": 3,
    "total_experience": 4.33
}