{
    "name": "SURAJ KANOJIYA",
    "email": "isurajk94@gmail.com",
    "mobile_number": "9170282378",
    "skills": [
        "Big data",
        "Hive",
        "Programming",
        "Technical",
        "Analysis",
        "Technical skills",
        "Engineering",
        "Json",
        "Windows",
        "Email",
        "Writing",
        "Sql",
        "Information technology",
        "Hadoop",
        "Scala",
        "Affiliate",
        "Queries",
        "Ansible",
        "Api",
        "Spark",
        "System",
        "Mysql",
        "Database",
        "Rest",
        "Java",
        "Linux",
        "Mobile"
    ],
    "college_name": null,
    "degree": [
        "Bachelor of Engineering- Information Technology from Atharva College of Engineering affiliate"
    ],
    "designation": null,
    "experience": [
        "Hadoop Developer, BigDataInt Engineers Pvt Ltd., Kochi [April 2019 \u2013 March 2020]",
        "Project profile",
        "1",
        "360 Dashboard",
        "Role: Hadoop Developer",
        "Tools and Technologies: Hadoop, Hive, Sqoop, Atlas, Java, Ranger, Spark, MYSQL,",
        "WebHCat.",
        "Developed end to end module for pulling metadata from HDP 2.6.5 cluster to HDP 3.1.4",
        "Atlas instance using big data (Hadoop, Hive, Sqoop, Atlas, Java, Ranger, Spark, MYSQL,",
        "WebHCat).",
        "\uf0b7 Created entities of hive-db, hive-table, hive-column in Apache Atlas which running in",
        "HDP 3.1.4.",
        "\uf0b7 Tested hive-hook and sqoop-hook.",
        "\uf0b7 Handle or how to hook tables created by Spark in Hive into Apache Atlas.",
        "\uf0b7",
        "\uf0b7 Creating tag-based policy in Atlas and implemented using Ranger.",
        "\uf0b7 Writing Ad-hoc queries to export metadata from Atlas using Rest API.",
        "Involved in developing crawler for gathering metadata of RDBMS into Hive.",
        "2 Black Diamond",
        "Role: Hadoop Developer",
        "Tools and Technologies: Hadoop, Spark, Spark Sql, Hive, Sqoop and MYSQL.",
        "An application responsible to analysis of transaction\u2019s data using Hadoop, Spark, Hive,",
        "Sqoop and MYSQL, which help to create data report of the client.",
        "Importing all the Customer specific data from MYSQL to Hadoop using Sqoop.",
        "\uf0b7",
        "\uf0b7 Analyzed data using Hadoop component hive.",
        "\uf0b7 Created Hive tables to store data and written Hive queries.",
        "\uf0b7 Loaded data into hive partitioned tables.",
        "\uf0b7",
        "\uf0b7 Writing and Optimizing hive queries",
        "\uf0b7 Performing Joins by using map-side join for faster analysis of hive tables.",
        "\uf0b7 Created dataframe from Json files in HDFS using Spark Sql.",
        "\uf0b7 Created document for same.",
        "Involved in incremental load and batch load into hive."
    ],
    "company_names": null,
    "no_of_pages": 2,
    "total_experience": 0.92
}