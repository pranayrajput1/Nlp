{
    "name": "Shantanu Parag",
    "email": "shantanubugadi007@gmail.com",
    "mobile_number": "9860699883",
    "skills": [
        "Hive",
        "Os",
        "Auditing",
        "Health",
        "Analytics",
        "Queries",
        "Engineering",
        "Linux",
        "Standardization",
        "Hadoop",
        "Config",
        "Oracle",
        "Windows",
        "Sql",
        "Java",
        "Sql server",
        "Shell",
        "Cloud",
        "Analysis",
        "Technical",
        "Research",
        "Email",
        "Python",
        "Json",
        "Design",
        "Aws",
        "Scripting",
        "Mobile",
        "Security",
        "Programming",
        "Spark",
        "Mysql",
        "Parser",
        "Migration",
        "Pyspark",
        "Certification",
        "Operating systems",
        "System"
    ],
    "college_name": null,
    "degree": [
        "B.E."
    ],
    "designation": [
        "Trainee Engineer"
    ],
    "experience": [
        "Oracle, Netezza and Teradata",
        "\u25cf Good knowledge on Java and Python programming languages.",
        "\u25cf Sound knowledge in",
        "\u25cf Effective team member skills, inspired and motivated towards work.",
        "\u25cf Quick learner and bendable enough to adapt any technology",
        "Data Structure and Algorithm Design",
        "to Hadoop using Sqoop.",
        "Experience:-",
        "\u25cf Working as  Engineer 1  for DataMetica Solutions Pvt Ltd Pune from DEC 2017 to till date.",
        "Technical Skills:-",
        "Cloud Platform",
        "Compute Instance,Composer",
        ",",
        "GCP, AWS ,Azure, Bigquery, Dataproc",
        "Hadoop Ecosystems",
        "Big Data Framework",
        "Scheduler",
        "Programming Languages",
        "Scripting",
        "Operating Systems",
        "Databases",
        "IDE & Editors",
        "PySpark,HDFS, Hive,Sqoop,MapReduce, Yarn, Pig",
        "Hadoop, Spark",
        "Azkaban, Autosys,Airflow,Composer,Control M",
        "Java (Core)",
        "Python,Shell",
        "Linux, Windows 7, Windows 10 ,Centos, Mac OS X",
        "Netezza,Sql Server , Teradata,Oracle , MySQL",
        "IntelliJ Idea, Eclipse, Notepad++",
        "Education:-",
        "DEGREE            : B.E.  E&TC",
        "COLLEGE",
        "YEAR                 : 2017",
        "PERCENTAGE  :  66.6%  (Distinction)",
        ":",
        "GOVERNMENT COLLEGE OF ENGINEERING AND RESEARCH , AWASARI (PUNE)",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "Projects:-",
        "Project Name",
        "5.",
        "Role",
        "Organization",
        "Team Size",
        "Technologies",
        "Project Description",
        "LIFEPOINT",
        "Engineer 1",
        "Datametica Solutions",
        "20",
        "Azure Synapse, Azure DataFactory ,SQL Server",
        "Lifepoint being a health based client wanted to build Analytics over Microsoft Azure using Azure",
        "DataFactory",
        "and Azure Synapse by pulling data from Sql Server",
        "Contribution",
        "\u25cf",
        "Complete ownership of new EMR development setting up pipeline ,created set up for 54 staging",
        "tables , developed 22 Stored Procedures , Written Business Validation Queries.",
        "\u25cf Analysed the existing system in SQL Server and figured out the logic in Stored Procedure.",
        "\u25cf Identified the reusability of the present SP while referring to the STM given by client .",
        "\u25cf Written Stored Procedures for the new module in project in Synapse.",
        "Project Name",
        "4.",
        "Role",
        "Organization",
        "Team Size",
        "Technologies",
        "Project Description",
        "SKY",
        "Engineer 1",
        "Datametica Solutions",
        "15",
        "BigQuery,Python ,Shell script, Compute Instance",
        "This project is to build analytics over GCP cloud  using Bigquery",
        "Contribution",
        "\u25cf Completed end to end Development ( from STM to Data Validation ) of Tuner Event Module.",
        "\u25cf Sql scripts parser to detect the source and target tables from the sql script for initial analysis.",
        "\u25cf Python script to validate file load in bq table comparing the file vs table records.",
        "\u25cf Written generic code to load data from gs bucket to respective tables",
        "\u25cf BQ auditing and housekeeping scripts",
        "Project Name",
        "3.",
        "Role",
        "Organization",
        "Team Size",
        "Technologies",
        "Project Description",
        "RMG",
        "Engineer 1",
        "Datametica Solutions",
        "12",
        "Datastage , BigQuery , Shell script, Compute Instance",
        "This project is regarding migration of datastage (Teradata) to GCP cloud (Big Query)",
        "Contribution",
        "\u25cf Written a generic code to load data from gs bucket to bq table",
        "\u25cf Converted bteq scripts to BQ compatible and wrapper for auditing and error handling",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u25cf Written python script for merging XSD files",
        "Project Name",
        "2.",
        "Role",
        "Organization",
        "Team Size",
        "Technologies",
        "Project Description",
        "Catalina",
        "Engineer 1",
        "Datametica Solutions",
        "9",
        "Spark,Hadoop, Python ,Hive, Sqoop, Shell script",
        "This project is regarding ingestion of workload from Netezza to Microsoft Azure Datalake.It also consisted of",
        "file ingestion of several format like CSV ,Fixed length, Json .Creation of Elastic Cluster for the balancing the",
        ".",
        "workload accordingly and allocating the nodes at runtime",
        "Contribution",
        "\u25cf Written sqoop jobs (Ingestion wrappers).",
        "\u25cf Written a python script for automatic DDL creation by hitting Netezza through sqoop eval.",
        "\u25cf Written Json file ingestion scripts with validation(Pyspark).",
        "\u25cf Hive Scripts  for transformation from one layer to another.",
        "\u25cf Automated sql queries run from hadoop to netezza with proper auditing",
        "Project Name",
        "1.",
        "Role",
        "Organization",
        "Team Size",
        "Technologies",
        "Project Description",
        "GCP Cloud Migration",
        "Trainee Engineer",
        "Datametica Solutions",
        "22",
        "Spark,Java, Pig, Hive, Shell Script",
        "The project was a lift and shift from On-Premises to GCP Cloud. It was a migration project with the",
        "refactoring of the whole code structure according to security standards.Making the code to be generic .",
        "Separating the variables in a config and the main scripts separately.",
        "Contribution",
        "\u25cf Migrated PII subject areas to GCP Cloud (DataProc Cluster)",
        "\u25cf Parameterized the scripts to make it complete generic",
        "\u25cf Written a python validation script which would validate the data after migration",
        "Courses and Achievements",
        ": -",
        "Course Name",
        "Organizer",
        "GCP Data Engineer Certification",
        "GOOGLE Certification",
        "Coursera GCP certification",
        "Coursera Certification",
        "Data Science Essentials",
        "Cloudera University",
        "Declaration",
        "The information provided above is the best of my knowledge and contains no faltered, modified or fabricated",
        "aspects about my profile and qualifications",
        "\u200b",
        "\u200b",
        "\u200b",
        "\u200b",
        "Place: Pune, India.",
        "Shantanu Bugadi"
    ],
    "company_names": [
        "Microsoft"
    ],
    "no_of_pages": 4,
    "total_experience": 0.0
}